{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4837752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='kalshi-456121'\n",
    "BUCKET_NAME='kalshi-data-lake'\n",
    "CLUSTER='kalshi-dataproc-cluster'\n",
    "REGION='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1ffa5",
   "metadata": {},
   "source": [
    "## Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b617b90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 16:06:58 WARN Utils: Your hostname, Jakes-MacBook-Pro-1815.local resolves to a loopback address: 127.0.0.1; using 100.73.104.202 instead (on interface en0)\n",
      "25/04/16 16:06:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/04/16 16:06:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------+--------------------+\n",
      "|              ticker|count|yes_price|        created_time|\n",
      "+--------------------+-----+---------+--------------------+\n",
      "|KXNASDAQ100U-25AP...|   50|       89|2025-04-16T19:41:...|\n",
      "|KXFEDDECISION-25M...|    5|       12|2025-04-16T19:41:...|\n",
      "|KXNBAGAME-25APR16...|  179|       53|2025-04-16T19:41:...|\n",
      "|KXFEDDECISION-25M...|    4|       12|2025-04-16T19:41:...|\n",
      "|KXNASDAQ100U-25AP...|   10|       55|2025-04-16T19:41:...|\n",
      "|KXBTCD-25APR1616-...|  184|       66|2025-04-16T19:41:...|\n",
      "|KXBTCD-25APR1616-...|   91|       64|2025-04-16T19:41:...|\n",
      "|KXSECPRESSMENTION...|  100|       35|2025-04-16T19:41:...|\n",
      "|KXSECPRESSMENTION...|   34|       33|2025-04-16T19:41:...|\n",
      "|KXSECPRESSMENTION...|   13|       33|2025-04-16T19:41:...|\n",
      "+--------------------+-----+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# GCS bucket path to the Parquet file or folder\n",
    "gcs_path = f\"gs://{BUCKET_NAME}/trades/historical/01_2025/trades_2025-04-16T19.parquet\"\n",
    "\n",
    "# Path to the GCS connector jar\n",
    "from os.path import expanduser\n",
    "gcs_connector_path = expanduser(\"~/Documents/gcloud/gcs-connector-hadoop3-latest.jar\")\n",
    "# Start a local Spark session with the GCS connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"QueryGCSParquet\") \\\n",
    "    .config(\"spark.jars\", gcs_connector_path) \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/Users/jakewatson/hello/kalshi/creds/gcp-sa-key.json\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"kalshi-456121\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Parquet file from GCS\n",
    "df = spark.read.parquet(gcs_path)\n",
    "\n",
    "# Perform a simple query\n",
    "df.select(\"ticker\", \"count\", \"yes_price\", \"created_time\").show(10)\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba8f6f",
   "metadata": {},
   "source": [
    "## Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a72e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ticker='KXETHD-25APR2515-T1279.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$1,280 or above', yes_sub_title='$1,280 or above', no_sub_title='$1,280 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 1279.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=1279.99, cap_strike=None)\n",
      "Row(ticker='KXETHD-25APR2515-T1259.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$1,260 or above', yes_sub_title='$1,260 or above', no_sub_title='$1,260 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 1259.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=1259.99, cap_strike=None)\n",
      "Row(ticker='KXETHD-25APR2515-T1239.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$1,240 or above', yes_sub_title='$1,240 or above', no_sub_title='$1,240 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 1239.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=1239.99, cap_strike=None)\n",
      "Row(ticker='KXETHD-25APR2515-T1219.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$1,220 or above', yes_sub_title='$1,220 or above', no_sub_title='$1,220 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 1219.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=1219.99, cap_strike=None)\n",
      "Row(ticker='KXETHD-25APR2515-T999.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$1,000 or above', yes_sub_title='$1,000 or above', no_sub_title='$1,000 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 999.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=999.99, cap_strike=None)\n",
      "Row(ticker='KXETHD-25APR2515-T979.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$980 or above', yes_sub_title='$980 or above', no_sub_title='$980 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 979.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=979.99, cap_strike=None)\n",
      "Row(ticker='KXETHD-25APR2515-T1199.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$1,200 or above', yes_sub_title='$1,200 or above', no_sub_title='$1,200 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 1199.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=1199.99, cap_strike=None)\n",
      "Row(ticker='KXETHD-25APR2515-T1179.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$1,180 or above', yes_sub_title='$1,180 or above', no_sub_title='$1,180 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 1179.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=1179.99, cap_strike=None)\n",
      "Row(ticker='KXETHD-25APR2515-T1159.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$1,160 or above', yes_sub_title='$1,160 or above', no_sub_title='$1,160 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 1159.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=1159.99, cap_strike=None)\n",
      "Row(ticker='KXETHD-25APR2515-T1139.99', event_ticker='KXETHD-25APR2515', market_type='binary', title='Ethereum price at Apr 25, 2025 at 3pm EDT?', subtitle='$1,140 or above', yes_sub_title='$1,140 or above', no_sub_title='$1,140 or above', open_time='2025-04-25T18:00:00Z', close_time='2025-04-25T19:00:00Z', expected_expiration_time='2025-04-25T19:05:00Z', expiration_time='2025-05-02T19:00:00Z', latest_expiration_time='2025-05-02T19:00:00Z', settlement_timer_seconds=60, status='initialized', response_price_units='usd_cent', notional_value=100, tick_size=1, yes_bid=0, yes_ask=0, no_bid=100, no_ask=100, last_price=0, previous_yes_bid=0, previous_yes_ask=0, previous_price=0, volume=0, volume_24h=0, liquidity=0, open_interest=0, result='', can_close_early=True, expiration_value='', category='', risk_limit_cents=0, strike_type='greater', custom_strike=None, rules_primary=\"If the simple average of the sixty seconds of CF Benchmarks' Ethereum Real-Time Index (ERTI) before 3 PM EDT is above 1139.99 at 3 PM EDT on Apr 25, 2025, then the market resolves to Yes.\", rules_secondary=\"Not all cryptocurrency price data is the same. While checking a source like Google or Coinbase may help guide your decision, the price used to determine this market is based on CF Benchmarks' corresponding Real Time Index (RTI). At the last minute before expiration, 60 RTI prices are collected. The official and final value is the average of these prices.\", day_str='2025-04-25', floor_strike=1139.99, cap_strike=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# GCS bucket path to the Parquet file or folder\n",
    "gcs_path = f\"gs://{BUCKET_NAME}/markets/01012025_to_04242025v2.parquet\"\n",
    "\n",
    "# Path to the GCS connector jar\n",
    "from os.path import expanduser\n",
    "gcs_connector_path = expanduser(\"~/Documents/gcloud/gcs-connector-hadoop3-latest.jar\")\n",
    "# Start a local Spark session with the GCS connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"QueryGCSParquet\") \\\n",
    "    .config(\"spark.jars\", gcs_connector_path) \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/Users/jakewatson/hello/kalshi/creds/gcp-sa-key.json\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"kalshi-456121\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Parquet file from GCS\n",
    "df = spark.read.parquet(gcs_path)\n",
    "\n",
    "# Perform a simple query\n",
    "last_10 = df.collect()[-10:]\n",
    "for row in last_10[:1]:\n",
    "    print(row)\n",
    "# df.show(10)\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e4ed5",
   "metadata": {},
   "source": [
    "## Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1321bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/24 11:55:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+--------------------+----------------------+------------------+--------------------+-----------+-------------+\n",
      "|        event_ticker|    series_ticker|           sub_title|               title|collateral_return_type|mutually_exclusive|            category|strike_date|strike_period|\n",
      "+--------------------+-----------------+--------------------+--------------------+----------------------+------------------+--------------------+-----------+-------------+\n",
      "|      KXROBOTMARS-35|      KXROBOTMARS|         Before 2035|Will a humanoid r...|                      |             false|Science and Techn...|       NULL|         NULL|\n",
      "|       KXNEXTPOPE-35|       KXNEXTPOPE|         Before 2035|Who will the next...|                MECNET|              true|               World|       NULL|         NULL|\n",
      "|    KXLALEADEROUT-35|    KXLALEADEROUT|         Before 2035|Which of these La...|                MECNET|              true|            Politics|       NULL|         NULL|\n",
      "|    KXG7LEADEROUT-35|    KXG7LEADEROUT|         Before 2035|Which of these G7...|                MECNET|              true|            Politics|       NULL|         NULL|\n",
      "|    KXEULEADEROUT-35|    KXEULEADEROUT|         Before 2035|Which of these Eu...|                MECNET|              true|            Politics|       NULL|         NULL|\n",
      "|       KXBRUVSEAT-35|       KXBRUVSEAT|In the next U.K. ...|Will Andrew Tate'...|                      |             false|           Elections|       NULL|         NULL|\n",
      "|  KXASIALEADEROUT-35|  KXASIALEADEROUT|         Before 2035|Which of these As...|                MECNET|              true|            Politics|       NULL|         NULL|\n",
      "|KXAFRICALEADEROUT-35|KXAFRICALEADEROUT|         Before 2035|Which of these Af...|                MECNET|              true|            Politics|       NULL|         NULL|\n",
      "|     JOBLESS-21AUG28|        KXJOBLESS|From Aug 22-28, 2021|Initial jobless c...|                      |             false|           Economics|       NULL|         NULL|\n",
      "|           EUCLIMATE|      KXEUCLIMATE|             By 2030|EU meets its 2030...|                      |             false| Climate and Weather|       NULL|         NULL|\n",
      "+--------------------+-----------------+--------------------+--------------------+----------------------+------------------+--------------------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# GCS bucket path to the Parquet file or folder\n",
    "gcs_path = f\"gs://{BUCKET_NAME}/events/all.parquet\"\n",
    "\n",
    "# Path to the GCS connector jar\n",
    "from os.path import expanduser\n",
    "gcs_connector_path = expanduser(\"~/Documents/gcloud/gcs-connector-hadoop3-latest.jar\")\n",
    "# Start a local Spark session with the GCS connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"QueryGCSParquet\") \\\n",
    "    .config(\"spark.jars\", gcs_connector_path) \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/Users/jakewatson/hello/kalshi/creds/gcp-sa-key.json\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"kalshi-456121\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Parquet file from GCS\n",
    "df = spark.read.parquet(gcs_path)\n",
    "\n",
    "# Perform a simple query\n",
    "df.show(10)\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745975e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can_close_early': True,\n",
      " 'category': '',\n",
      " 'close_time': '2026-12-31T15:00:00Z',\n",
      " 'custom_strike': {'Candidate': 'Mike Lawler'},\n",
      " 'event_ticker': 'KXGOVNYNOMR-26',\n",
      " 'expected_expiration_time': '2026-12-31T15:00:00Z',\n",
      " 'expiration_time': '2026-12-31T15:00:00Z',\n",
      " 'expiration_value': '',\n",
      " 'last_price': 0,\n",
      " 'latest_expiration_time': '2026-12-31T15:00:00Z',\n",
      " 'liquidity': 0,\n",
      " 'market_type': 'binary',\n",
      " 'no_ask': 100,\n",
      " 'no_bid': 100,\n",
      " 'no_sub_title': 'Mike Lawler',\n",
      " 'notional_value': 100,\n",
      " 'open_interest': 0,\n",
      " 'open_time': '2025-04-17T14:00:00Z',\n",
      " 'previous_price': 0,\n",
      " 'previous_yes_ask': 0,\n",
      " 'previous_yes_bid': 0,\n",
      " 'response_price_units': 'usd_cent',\n",
      " 'result': '',\n",
      " 'risk_limit_cents': 0,\n",
      " 'rules_primary': 'If Mike Lawler wins the nomination for the Republican Party '\n",
      "                  'to contest the 2026 New York Governorship, then the market '\n",
      "                  'resolves to Yes.',\n",
      " 'rules_secondary': '',\n",
      " 'settlement_timer_seconds': 300,\n",
      " 'status': 'initialized',\n",
      " 'strike_type': 'custom',\n",
      " 'subtitle': '',\n",
      " 'tick_size': 1,\n",
      " 'ticker': 'KXGOVNYNOMR-26-MLAW',\n",
      " 'title': 'Wil Mike Lawler be the Republican nominee for Governor in New York?',\n",
      " 'volume': 0,\n",
      " 'volume_24h': 0,\n",
      " 'yes_ask': 0,\n",
      " 'yes_bid': 0,\n",
      " 'yes_sub_title': 'Mike Lawler'}\n",
      "dict_keys(['ticker', 'event_ticker', 'market_type', 'title', 'subtitle', 'yes_sub_title', 'no_sub_title', 'open_time', 'close_time', 'expected_expiration_time', 'expiration_time', 'latest_expiration_time', 'settlement_timer_seconds', 'status', 'response_price_units', 'notional_value', 'tick_size', 'yes_bid', 'yes_ask', 'no_bid', 'no_ask', 'last_price', 'previous_yes_bid', 'previous_yes_ask', 'previous_price', 'volume', 'volume_24h', 'liquidity', 'open_interest', 'result', 'can_close_early', 'expiration_value', 'category', 'risk_limit_cents', 'strike_type', 'custom_strike', 'rules_primary', 'rules_secondary'])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pprint\n",
    "\n",
    "def fetch_markets():\n",
    "    url = \"https://api.elections.kalshi.com/trade-api/v2/markets\"\n",
    "    r = requests.get(url)\n",
    "    markets = r.json().get(\"markets\", [])\n",
    "    print(markets[0])\n",
    "    print(markets[0]['status'])\n",
    "    print(\"Keys: \", markets[0].keys())\n",
    "\n",
    "fetch_markets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8ebb3",
   "metadata": {},
   "source": [
    "## Clean and push to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e865678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+---------+--------+----------+----------+------------+\n",
      "|            trade_id|              ticker|count|        created_time|yes_price|no_price|taker_side|bid_amount|created_date|\n",
      "+--------------------+--------------------+-----+--------------------+---------+--------+----------+----------+------------+\n",
      "|13e20041-a226-47f...|KXWMARMAD-25R64G1...|   73|2025-03-21T23:59:...|       99|       1|        no|      0.73|  2025-03-21|\n",
      "|40d20184-6d4f-4d3...|KXMARMAD-25R64G7-...|    5|2025-03-21T23:59:...|       49|      51|       yes|      2.45|  2025-03-21|\n",
      "|48e5bf41-187a-44f...|KXMARMAD-25R64G30-UK|   25|2025-03-21T23:59:...|       92|       8|        no|       2.0|  2025-03-21|\n",
      "|388abf18-291b-4dc...|KXMARMAD-25R64G7-UNM|  234|2025-03-21T23:59:...|       53|      47|       yes|    124.02|  2025-03-21|\n",
      "|980f8084-2678-419...|   KXELECTUKRAINE-26|  207|2025-03-21T23:59:...|       41|      59|        no|    122.13|  2025-03-21|\n",
      "|c74e7de2-7a21-410...|KXMARMAD-25R64G30...|  405|2025-03-21T23:59:...|        8|      92|       yes|      32.4|  2025-03-21|\n",
      "|0bdae6dc-47d8-4cb...|KXMARMAD-25R64G7-UNM|  200|2025-03-21T23:59:...|       53|      47|       yes|     106.0|  2025-03-21|\n",
      "|cb23b998-544e-4d5...|KXMARMAD-25R64G7-UNM|   80|2025-03-21T23:59:...|       53|      47|       yes|      42.4|  2025-03-21|\n",
      "|d366edce-e337-408...|KXWMARMAD-25R64G2...|   20|2025-03-21T23:59:...|       55|      45|        no|       9.0|  2025-03-21|\n",
      "|1cc0e993-7658-4a0...|KXMARMAD-25R32G11...|  100|2025-03-21T23:59:...|       53|      47|       yes|      53.0|  2025-03-21|\n",
      "+--------------------+--------------------+-----+--------------------+---------+--------+----------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 14:49:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/25 14:49:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/25 14:49:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/25 14:49:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 53:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o9608.parquet.\n: java.io.IOException: Failed to get result: java.io.IOException: Error accessing Bucket kalshi-data-lake with message : java.io.IOException: Error accessing Bucket kalshi-data-lake\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFromFuture(GoogleCloudStorageFileSystem.java:1047)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.repairImplicitDirectory(GoogleCloudStorageFileSystem.java:1017)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.delete(GoogleCloudStorageFileSystem.java:457)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$delete$7(GoogleHadoopFileSystemBase.java:918)\n\tat com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics.trackDuration(GhfsStorageStatistics.java:104)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.delete(GoogleHadoopFileSystemBase.java:906)\n\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:183)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:238)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.util.concurrent.ExecutionException: java.io.IOException: Error accessing Bucket kalshi-data-lake\n\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFromFuture(GoogleCloudStorageFileSystem.java:1040)\n\t... 52 more\nCaused by: java.io.IOException: Error accessing Bucket kalshi-data-lake\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getBucket(GoogleCloudStorageImpl.java:2280)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2220)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1226)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.lambda$delete$2(GoogleCloudStorageFileSystem.java:391)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/kalshi-data-lake\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"kalshi-data-loader@kalshi-456121.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).\",\n    \"reason\" : \"forbidden\"\n  } ],\n  \"message\" : \"kalshi-data-loader@kalshi-456121.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getBucket(GoogleCloudStorageImpl.java:2273)\n\t... 7 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 143\u001b[39m\n\u001b[32m     91\u001b[39m joined_df = aggs_df.join(markets_df.alias(\u001b[33m\"\u001b[39m\u001b[33mm\u001b[39m\u001b[33m\"\u001b[39m), on=[\u001b[33m\"\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m\"\u001b[39m], how=\u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     92\u001b[39m                     .join(events_df.alias(\u001b[33m\"\u001b[39m\u001b[33me\u001b[39m\u001b[33m\"\u001b[39m), on=col(\u001b[33m\"\u001b[39m\u001b[33mm_event_ticker\u001b[39m\u001b[33m\"\u001b[39m) == col(\u001b[33m\"\u001b[39m\u001b[33mevent_ticker\u001b[39m\u001b[33m\"\u001b[39m), how=\u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     93\u001b[39m                     .drop(\u001b[33m\"\u001b[39m\u001b[33mm_event_ticker\u001b[39m\u001b[33m\"\u001b[39m)              \n\u001b[32m     95\u001b[39m joined_df = joined_df.select(\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# Identifiers\u001b[39;00m\n\u001b[32m     97\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m3_day_no_momentum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    141\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[43mjoined_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgs://kalshi-data-lake/processed_data/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# joined_df.write.mode(\"overwrite\").parquet(\"gs://kalshi-data-lake/processed_data/\")\u001b[39;00m\n\u001b[32m    146\u001b[39m joined_df.filter(col(\u001b[33m\"\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mKXFEDDECISION-25MAY-C25\u001b[39m\u001b[33m\"\u001b[39m).show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/apis/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1721\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   1719\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   1720\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m1721\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/apis/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/apis/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/apis/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o9608.parquet.\n: java.io.IOException: Failed to get result: java.io.IOException: Error accessing Bucket kalshi-data-lake with message : java.io.IOException: Error accessing Bucket kalshi-data-lake\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFromFuture(GoogleCloudStorageFileSystem.java:1047)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.repairImplicitDirectory(GoogleCloudStorageFileSystem.java:1017)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.delete(GoogleCloudStorageFileSystem.java:457)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$delete$7(GoogleHadoopFileSystemBase.java:918)\n\tat com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics.trackDuration(GhfsStorageStatistics.java:104)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.delete(GoogleHadoopFileSystemBase.java:906)\n\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:183)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:238)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.util.concurrent.ExecutionException: java.io.IOException: Error accessing Bucket kalshi-data-lake\n\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFromFuture(GoogleCloudStorageFileSystem.java:1040)\n\t... 52 more\nCaused by: java.io.IOException: Error accessing Bucket kalshi-data-lake\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getBucket(GoogleCloudStorageImpl.java:2280)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2220)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1226)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.lambda$delete$2(GoogleCloudStorageFileSystem.java:391)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/kalshi-data-lake\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"kalshi-data-loader@kalshi-456121.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).\",\n    \"reason\" : \"forbidden\"\n  } ],\n  \"message\" : \"kalshi-data-loader@kalshi-456121.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getBucket(GoogleCloudStorageImpl.java:2273)\n\t... 7 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import last, when, col, lag, round, avg, expr, to_date, sum as spark_sum, monotonically_increasing_id\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from os.path import expanduser\n",
    "gcs_connector_path = expanduser(\"~/Documents/gcloud/gcs-connector-hadoop3-latest.jar\")\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JoinAndAggregateKalshi\") \\\n",
    "    .config(\"spark.jars\", gcs_connector_path) \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/Users/jakewatson/hello/kalshi/creds/gcp-sa-key.json\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"kalshi-456121\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "# ---------------- Trades ----------------- #    \n",
    "import os\n",
    "folder_path = f\"gs://{BUCKET_NAME}/trades/\"\n",
    "\n",
    "# Read them into one DataFrame\n",
    "trades_df = spark.read.parquet(f\"gs://{BUCKET_NAME}/trades/*.parquet\")\n",
    "trades_df = trades_df.withColumn(\n",
    "    \"bid_amount\", \n",
    "    round(\n",
    "        when(col(\"taker_side\") == \"yes\", col(\"yes_price\") * col(\"count\") / 100)\n",
    "        .when(col(\"taker_side\") == \"no\", (col(\"no_price\") * col(\"count\") / 100))\n",
    "        .otherwise(0), 2\n",
    "    )\n",
    ")\n",
    "trades_df = trades_df.withColumn(\"created_date\", to_date(\"created_time\"))\n",
    "trades_df.show(10)\n",
    "\n",
    "# # Define a window by market_ticker, ordered by trade creation time\n",
    "rolling_7_day_window = Window.partitionBy(\"ticker\").orderBy(\"created_date\").rowsBetween(-6, 0)\n",
    "rolling_3_day_window = Window.partitionBy(\"ticker\").orderBy(\"created_date\").rowsBetween(-2, 0)\n",
    "price_window = Window.partitionBy(\"ticker\", \"created_date\").orderBy(col(\"created_time\")).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "momentum_window = Window.partitionBy(\"ticker\").orderBy(\"created_date\")\n",
    "\n",
    "# Apply last with enforced ordering using window\n",
    "trades_df = trades_df.withColumn(\"yes_price_close\", last(when(col(\"taker_side\") == \"yes\", col(\"yes_price\")), True).over(price_window)) \\\n",
    "                     .withColumn(\"no_price_close\", last(when(col(\"taker_side\") == \"no\", col(\"no_price\")), True).over(price_window))\n",
    "\n",
    "# ---------------- Trade Aggregations / Indicators ----------------- #                      \n",
    "aggs_df = trades_df.groupBy(\n",
    "    \"created_date\",\n",
    "    \"ticker\",\n",
    "    \"yes_price_close\",\n",
    "    \"no_price_close\", \n",
    ").agg(\n",
    "    spark_sum(when(col(\"taker_side\") == \"yes\", col(\"bid_amount\")).otherwise(0)).alias(\"daily_total_bid_amount_yes\"),\n",
    "    spark_sum(when(col(\"taker_side\") == \"no\", col(\"bid_amount\")).otherwise(0)).alias(\"daily_total_bid_amount_no\"),\n",
    "    spark_sum(\"bid_amount\").alias(\"daily_total_bid_amount\")\n",
    ")\n",
    "\n",
    "aggs_df = aggs_df.withColumn(\"7_day_avg_invested_amount\", round(avg(\"daily_total_bid_amount\").over(rolling_7_day_window),2)) \\\n",
    "                .withColumn(\"7_day_avg_yes_invested_amount\", round(avg(\"daily_total_bid_amount_yes\").over(rolling_7_day_window),2)) \\\n",
    "                .withColumn(\"7_day_avg_no_invested_amount\", round(avg(\"daily_total_bid_amount_no\").over(rolling_7_day_window),2)) \\\n",
    "                .withColumn(\"daily_total_vs_7_day_avg\", col(\"daily_total_bid_amount\") - round(avg(\"daily_total_bid_amount\").over(rolling_7_day_window),2)) \\\n",
    "                .withColumn(\"daily_yes_total_vs_7_day_avg\", col(\"daily_total_bid_amount_yes\") - round(avg(\"daily_total_bid_amount_yes\").over(rolling_7_day_window),2)) \\\n",
    "                .withColumn(\"daily_no_total_vs_7_day_avg\", col(\"daily_total_bid_amount_no\") - round(avg(\"daily_total_bid_amount_no\").over(rolling_7_day_window),2)) \\\n",
    "                .withColumn(\"7_day_yes_momentum\", col(\"yes_price_close\") - lag(\"yes_price_close\", 7).over(momentum_window)) \\\n",
    "                .withColumn(\"7_day_no_momentum\", col(\"no_price_close\") - lag(\"no_price_close\", 7).over(momentum_window)) \\\n",
    "                .withColumn(\"3_day_avg_invested_amount\", round(avg(\"daily_total_bid_amount\").over(rolling_3_day_window),2)) \\\n",
    "                .withColumn(\"3_day_avg_yes_invested_amount\", round(avg(\"daily_total_bid_amount_yes\").over(rolling_3_day_window),2)) \\\n",
    "                .withColumn(\"3_day_avg_no_invested_amount\", round(avg(\"daily_total_bid_amount_no\").over(rolling_3_day_window),2)) \\\n",
    "                .withColumn(\"daily_total_vs_3_day_avg\", col(\"daily_total_bid_amount\") - round(avg(\"daily_total_bid_amount\").over(rolling_3_day_window),2)) \\\n",
    "                .withColumn(\"daily_yes_total_vs_3_day_avg\", col(\"daily_total_bid_amount_yes\") - round(avg(\"daily_total_bid_amount_yes\").over(rolling_3_day_window),2)) \\\n",
    "                .withColumn(\"daily_no_total_vs_3_day_avg\", col(\"daily_total_bid_amount_no\") - round(avg(\"daily_total_bid_amount_no\").over(rolling_3_day_window),2)) \\\n",
    "                .withColumn(\"3_day_yes_momentum\", col(\"yes_price_close\") - lag(\"yes_price_close\", 3).over(momentum_window)) \\\n",
    "                .withColumn(\"3_day_no_momentum\", col(\"no_price_close\") - lag(\"no_price_close\", 3).over(momentum_window))\n",
    "\n",
    "# aggs_df.filter(col(\"ticker\") == \"KXFEDDECISION-25MAY-C25\").show()\n",
    "\n",
    "# ---------------- Markets, Events ----------------- #                      \n",
    "markets_df = spark.read.parquet(f\"gs://{BUCKET_NAME}/markets/01012025_to_04242025.parquet\")\n",
    "markets_df = markets_df.select(\"ticker\", col(\"event_ticker\").alias(\"m_event_ticker\"), col(\"title\").alias(\"market_title\"), col(\"subtitle\").alias(\"market_subtitle\"), col(\"rules_primary\").alias(\"market_desc\"), \"open_time\", \"close_time\", \"market_type\")\n",
    "\n",
    "events_df = spark.read.parquet(f\"gs://{BUCKET_NAME}/events/all.parquet\")\n",
    "events_df = events_df.select(\"event_ticker\", \"series_ticker\", col(\"title\").alias(\"event_title\"), col(\"sub_title\").alias(\"event_subtitle\"), \"category\")\n",
    "\n",
    "# ---------------- Joined ----------------- #   \n",
    "joined_df = aggs_df.join(markets_df.alias(\"m\"), on=[\"ticker\"], how=\"inner\") \\\n",
    "                    .join(events_df.alias(\"e\"), on=col(\"m_event_ticker\") == col(\"event_ticker\"), how=\"inner\") \\\n",
    "                    .drop(\"m_event_ticker\")              \n",
    "\n",
    "joined_df = joined_df.select(\n",
    "    # Identifiers\n",
    "    \"ticker\",\n",
    "    \"event_ticker\",\n",
    "    \"series_ticker\",\n",
    "    \"created_date\",\n",
    "\n",
    "    # Market/Event Info\n",
    "    \"event_title\",\n",
    "    \"market_title\",\n",
    "    \"market_subtitle\",\n",
    "    \"event_subtitle\",\n",
    "    \"market_desc\",\n",
    "    \"open_time\",\n",
    "    \"close_time\",\n",
    "    \"market_type\",\n",
    "    \"category\",\n",
    "\n",
    "    # Prices\n",
    "    \"yes_price_close\",\n",
    "    \"no_price_close\",\n",
    "\n",
    "    # Daily Totals\n",
    "    \"daily_total_bid_amount_yes\",\n",
    "    \"daily_total_bid_amount_no\",\n",
    "    \"daily_total_bid_amount\",\n",
    "\n",
    "    # 7-Day Averages\n",
    "    \"7_day_avg_invested_amount\",\n",
    "    \"7_day_avg_yes_invested_amount\",\n",
    "    \"7_day_avg_no_invested_amount\",\n",
    "    \"daily_total_vs_7_day_avg\",\n",
    "    \"daily_yes_total_vs_7_day_avg\",\n",
    "    \"daily_no_total_vs_7_day_avg\",\n",
    "    \"7_day_yes_momentum\",\n",
    "    \"7_day_no_momentum\",\n",
    "\n",
    "    # 3-Day Averages\n",
    "    \"3_day_avg_invested_amount\",\n",
    "    \"3_day_avg_yes_invested_amount\",\n",
    "    \"3_day_avg_no_invested_amount\",\n",
    "    \"daily_total_vs_3_day_avg\",\n",
    "    \"daily_yes_total_vs_3_day_avg\",\n",
    "    \"daily_no_total_vs_3_day_avg\",\n",
    "    \"3_day_yes_momentum\",\n",
    "    \"3_day_no_momentum\"\n",
    ")\n",
    "\n",
    "joined_df.repartition(10).write.mode(\"overwrite\").parquet(\"gs://kalshi-data-lake/processed_data/\")\n",
    "# joined_df.write.mode(\"overwrite\").parquet(\"gs://kalshi-data-lake/processed_data/\")\n",
    "\n",
    "joined_df.filter(col(\"ticker\") == \"KXFEDDECISION-25MAY-C25\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0024025",
   "metadata": {},
   "source": [
    "## Read Cleaned Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bea101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223593\n",
      "root\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- created_date: date (nullable = true)\n",
      " |-- yes_price_close: long (nullable = true)\n",
      " |-- no_price_close: long (nullable = true)\n",
      " |-- daily_total_bid_amount_yes: double (nullable = true)\n",
      " |-- daily_total_bid_amount_no: double (nullable = true)\n",
      " |-- daily_total_bid_amount: double (nullable = true)\n",
      " |-- 7_day_avg_invested_amount: double (nullable = true)\n",
      " |-- 7_day_avg_yes_invested_amount: double (nullable = true)\n",
      " |-- 7_day_avg_no_invested_amount: double (nullable = true)\n",
      " |-- daily_total_vs_7_day_avg: double (nullable = true)\n",
      " |-- daily_yes_total_vs_7_day_avg: double (nullable = true)\n",
      " |-- daily_no_total_vs_7_day_avg: double (nullable = true)\n",
      " |-- 7_day_yes_momentum: long (nullable = true)\n",
      " |-- 7_day_no_momentum: long (nullable = true)\n",
      " |-- 3_day_avg_invested_amount: double (nullable = true)\n",
      " |-- 3_day_avg_yes_invested_amount: double (nullable = true)\n",
      " |-- 3_day_avg_no_invested_amount: double (nullable = true)\n",
      " |-- daily_total_vs_3_day_avg: double (nullable = true)\n",
      " |-- daily_yes_total_vs_3_day_avg: double (nullable = true)\n",
      " |-- daily_no_total_vs_3_day_avg: double (nullable = true)\n",
      " |-- 3_day_yes_momentum: long (nullable = true)\n",
      " |-- 3_day_no_momentum: long (nullable = true)\n",
      " |-- market_title: string (nullable = true)\n",
      " |-- market_subtitle: string (nullable = true)\n",
      " |-- open_time: string (nullable = true)\n",
      " |-- close_time: string (nullable = true)\n",
      " |-- market_type: string (nullable = true)\n",
      " |-- event_ticker: string (nullable = true)\n",
      " |-- series_ticker: string (nullable = true)\n",
      " |-- event_title: string (nullable = true)\n",
      " |-- event_subtitle: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---------------+--------------+--------------------------+-------------------------+----------------------+-------------------------+-----------------------------+----------------------------+------------------------+----------------------------+---------------------------+------------------+-----------------+-------------------------+-----------------------------+----------------------------+------------------------+----------------------------+---------------------------+------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|              ticker|created_date|yes_price_close|no_price_close|daily_total_bid_amount_yes|daily_total_bid_amount_no|daily_total_bid_amount|7_day_avg_invested_amount|7_day_avg_yes_invested_amount|7_day_avg_no_invested_amount|daily_total_vs_7_day_avg|daily_yes_total_vs_7_day_avg|daily_no_total_vs_7_day_avg|7_day_yes_momentum|7_day_no_momentum|3_day_avg_invested_amount|3_day_avg_yes_invested_amount|3_day_avg_no_invested_amount|daily_total_vs_3_day_avg|daily_yes_total_vs_3_day_avg|daily_no_total_vs_3_day_avg|3_day_yes_momentum|3_day_no_momentum|        market_title|     market_subtitle|           open_time|          close_time|market_type|        event_ticker|series_ticker|         event_title|      event_subtitle|  category|\n",
      "+--------------------+------------+---------------+--------------+--------------------------+-------------------------+----------------------+-------------------------+-----------------------------+----------------------------+------------------------+----------------------------+---------------------------+------------------+-----------------+-------------------------+-----------------------------+----------------------------+------------------------+----------------------------+---------------------------+------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|KXINXY-25DEC31-B6700|  2025-04-21|              5|            95|                       5.0|                     2.85|                  7.85|                   126.48|                          9.0|                      117.48|     -118.63000000000001|                        -4.0|        -114.63000000000001|              NULL|                3|                    17.14|                        11.67|                        5.48|      -9.290000000000001|                       -6.67|        -2.6300000000000003|              NULL|               -3|Will the S&P 500 ...|   6,600 to 6,799.99|2024-11-05T15:00:00Z|2025-12-31T15:00:00Z|     binary|      KXINXY-25DEC31|       KXINXY|S&P close price e...|     On Dec 31, 2025|Financials|\n",
      "|KXNASDAQ100U-25AP...|  2025-04-02|           NULL|            99|                       0.0|                  2537.95|               2537.95|                  2537.95|                          0.0|                     2537.95|                     0.0|                         0.0|                        0.0|              NULL|             NULL|                  2537.95|                          0.0|                     2537.95|                     0.0|                         0.0|                        0.0|              NULL|             NULL|Will the Nasdaq-1...|     19,470 or above|2025-04-02T14:00:00Z|2025-04-02T15:00:00Z|     binary|KXNASDAQ100U-25AP...| KXNASDAQ100U|Nasdaq price on A...|On Apr 2, 2025 at...|Financials|\n",
      "|KXINXU-25MAR03H15...|  2025-03-03|             27|          NULL|                      16.2|                      0.0|                  16.2|                     16.2|                         16.2|                         0.0|                     0.0|                         0.0|                        0.0|              NULL|             NULL|                     16.2|                         16.2|                         0.0|                     0.0|                         0.0|                        0.0|              NULL|             NULL|Will the S&P 500 ...| 5,920.0099 or above|2025-03-03T19:00:00Z|2025-03-03T20:00:00Z|     binary| KXINXU-25MAR03H1500|       KXINXU|S&P price on Mar ...|On Mar 3, 2025 at...|Financials|\n",
      "|KXBTC-25JAN2019-B...|  2025-01-20|             47|            98|        13.149999999999999|                    56.13|                 69.28|                    69.28|                        13.15|                       56.13|                     0.0|        -1.77635683940025...|                        0.0|              NULL|             NULL|                    69.28|                        13.15|                       56.13|                     0.0|        -1.77635683940025...|                        0.0|              NULL|             NULL|Bitcoin price ran...|$103,250 to 103,4...|2025-01-20T23:00:00Z|2025-01-21T00:00:00Z|     binary|     KXBTC-25JAN2019|        KXBTC|Bitcoin price ran...|On Jan 20, 2025 a...|    Crypto|\n",
      "|   KXVOTETULSI-26-TY|  2025-01-28|             63|            38|         6041.680000000001|                   423.99|     6465.670000000001|                  1177.88|                       995.36|                      182.52|       5287.790000000001|          5046.3200000000015|                     241.47|               -12|             NULL|                  2593.13|                      2210.36|                      382.77|       3872.540000000001|           3831.320000000001|          41.22000000000003|                -7|                4|Will Todd Young v...|                    |2024-12-18T21:00:25Z|2025-02-12T23:30:...|     binary|      KXVOTETULSI-26|  KXVOTETULSI|Which Senators wi...|         Before 2026|  Politics|\n",
      "+--------------------+------------+---------------+--------------+--------------------------+-------------------------+----------------------+-------------------------+-----------------------------+----------------------------+------------------------+----------------------------+---------------------------+------------------+-----------------+-------------------------+-----------------------------+----------------------------+------------------------+----------------------------+---------------------------+------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# GCS bucket path to the Parquet file or folder\n",
    "gcs_path = f\"gs://{BUCKET_NAME}/processed_data/\"\n",
    "\n",
    "# Path to the GCS connector jar\n",
    "from os.path import expanduser\n",
    "gcs_connector_path = expanduser(\"~/Documents/gcloud/gcs-connector-hadoop3-latest.jar\")\n",
    "# Start a local Spark session with the GCS connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"QueryGCSParquet\") \\\n",
    "    .config(\"spark.jars\", gcs_connector_path) \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/Users/jakewatson/hello/kalshi/creds/gcp-sa-key.json\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"kalshi-456121\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Parquet file from GCS\n",
    "df = spark.read.parquet(f\"{gcs_path}/*.parquet\")\n",
    "\n",
    "print(\"Records: \", df.count())\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e7d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
